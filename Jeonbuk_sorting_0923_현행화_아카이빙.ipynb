{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ade96d4-730a-46a8-a031-b4b1587f7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['part-r-00000.gz', 'part-r-00001.gz', 'part-r-00002.gz', 'part-r-00003.gz', 'part-r-00004.gz', 'part-r-00005.gz', 'part-r-00006.gz', 'part-r-00007.gz', 'part-r-00008.gz', 'part-r-00009.gz', 'part-r-00010.gz', 'part-r-00011.gz', 'part-r-00012.gz', 'part-r-00013.gz', 'part-r-00014.gz', 'part-r-00015.gz', 'part-r-00016.gz', 'part-r-00017.gz', 'part-r-00018.gz', 'part-r-00019.gz', 'part-r-00020.gz', 'part-r-00021.gz', 'part-r-00022.gz', 'part-r-00023.gz', 'part-r-00024.gz', 'part-r-00025.gz', 'part-r-00026.gz', 'part-r-00027.gz', 'part-r-00028.gz', 'part-r-00029.gz', 'part-r-00030.gz', 'part-r-00031.gz', 'part-r-00032.gz', 'part-r-00033.gz', 'part-r-00034.gz', 'part-r-00035.gz', 'part-r-00036.gz', 'part-r-00037.gz', 'part-r-00038.gz', 'part-r-00039.gz', 'part-r-00040.gz', 'part-r-00041.gz', 'part-r-00042.gz', 'part-r-00043.gz', 'part-r-00044.gz', 'part-r-00045.gz', 'part-r-00046.gz', 'part-r-00047.gz', 'part-r-00048.gz', 'part-r-00049.gz', 'part-r-00050.gz', 'part-r-00051.gz', 'part-r-00052.gz', 'part-r-00053.gz', 'part-r-00054.gz', 'part-r-00055.gz', 'part-r-00056.gz', 'part-r-00057.gz', 'part-r-00058.gz', 'part-r-00059.gz', 'part-r-00060.gz', 'part-r-00061.gz', 'part-r-00062.gz', 'part-r-00063.gz', 'part-r-00064.gz', 'part-r-00065.gz', 'part-r-00066.gz', 'part-r-00067.gz', 'part-r-00068.gz', 'part-r-00069.gz', 'part-r-00070.gz', 'part-r-00071.gz', 'part-r-00072.gz', 'part-r-00073.gz', 'part-r-00074.gz', 'part-r-00075.gz', 'part-r-00076.gz', 'part-r-00077.gz', 'part-r-00078.gz', 'part-r-00079.gz', 'part-r-00080.gz', 'part-r-00081.gz', 'part-r-00082.gz', 'part-r-00083.gz', 'part-r-00084.gz', 'part-r-00085.gz', 'part-r-00086.gz', 'part-r-00087.gz', 'part-r-00088.gz', 'part-r-00089.gz', 'part-r-00090.gz', 'part-r-00091.gz', 'part-r-00092.gz', 'part-r-00093.gz', 'part-r-00094.gz', 'part-r-00095.gz', 'part-r-00096.gz', 'part-r-00097.gz', 'part-r-00098.gz', 'part-r-00099.gz', 'part-r-00100.gz', 'part-r-00101.gz', 'part-r-00102.gz', 'part-r-00103.gz', 'part-r-00104.gz', 'part-r-00105.gz', 'part-r-00106.gz', 'part-r-00107.gz', 'part-r-00108.gz', 'part-r-00109.gz', 'part-r-00110.gz', 'part-r-00111.gz', 'part-r-00112.gz', 'part-r-00113.gz', 'part-r-00114.gz', 'part-r-00115.gz', 'part-r-00116.gz', 'part-r-00117.gz', 'part-r-00118.gz', 'part-r-00119.gz']\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# FTP 연결 설정\n",
    "ftp_host = 'varodrt.synology.me'\n",
    "ftp_port = 21\n",
    "ftp_user = '비밀'\n",
    "ftp_password = '비밀' #암호화\n",
    "\n",
    "# 서버 접속\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ftp_host, ftp_port)\n",
    "ftp.encoding = 'utf-8'\n",
    "ftp.sendcmd('OPTS UTF8 ON')  # 한글 사용을 위해 필수로 입력할 것\n",
    "ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "\n",
    "# 디렉토리 설정 및 파일 목록 가져오기\n",
    "ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/압축파일')\n",
    "file_list = ftp.nlst()  # 모든 파일 목록을 가져옴\n",
    "gz_files = [file.strip() for file in file_list if file.endswith('.gz')]  # .gz 파일만 필터링\n",
    "\n",
    "print(f\"Files found: {gz_files}\")  # 파일 목록 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb38d1b-4209-4fda-b340-1b20db8cc564",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['part-r-00000.gz', 'part-r-00001.gz', 'part-r-00002.gz', 'part-r-00003.gz', 'part-r-00004.gz', 'part-r-00005.gz', 'part-r-00006.gz', 'part-r-00007.gz', 'part-r-00008.gz', 'part-r-00009.gz', 'part-r-00010.gz', 'part-r-00011.gz', 'part-r-00012.gz', 'part-r-00013.gz', 'part-r-00014.gz', 'part-r-00015.gz', 'part-r-00016.gz', 'part-r-00017.gz', 'part-r-00018.gz', 'part-r-00019.gz', 'part-r-00020.gz', 'part-r-00021.gz', 'part-r-00022.gz', 'part-r-00023.gz', 'part-r-00024.gz', 'part-r-00025.gz', 'part-r-00026.gz', 'part-r-00027.gz', 'part-r-00028.gz', 'part-r-00029.gz', 'part-r-00030.gz', 'part-r-00031.gz', 'part-r-00032.gz', 'part-r-00033.gz', 'part-r-00034.gz', 'part-r-00035.gz', 'part-r-00036.gz', 'part-r-00037.gz', 'part-r-00038.gz', 'part-r-00039.gz', 'part-r-00040.gz', 'part-r-00041.gz', 'part-r-00042.gz', 'part-r-00043.gz', 'part-r-00044.gz', 'part-r-00045.gz', 'part-r-00046.gz', 'part-r-00047.gz', 'part-r-00048.gz', 'part-r-00049.gz', 'part-r-00050.gz', 'part-r-00051.gz', 'part-r-00052.gz', 'part-r-00053.gz', 'part-r-00054.gz', 'part-r-00055.gz', 'part-r-00056.gz', 'part-r-00057.gz', 'part-r-00058.gz', 'part-r-00059.gz', 'part-r-00060.gz', 'part-r-00061.gz', 'part-r-00062.gz', 'part-r-00063.gz', 'part-r-00064.gz', 'part-r-00065.gz', 'part-r-00066.gz', 'part-r-00067.gz', 'part-r-00068.gz', 'part-r-00069.gz', 'part-r-00070.gz', 'part-r-00071.gz', 'part-r-00072.gz', 'part-r-00073.gz', 'part-r-00074.gz', 'part-r-00075.gz', 'part-r-00076.gz', 'part-r-00077.gz', 'part-r-00078.gz', 'part-r-00079.gz', 'part-r-00080.gz', 'part-r-00081.gz', 'part-r-00082.gz', 'part-r-00083.gz', 'part-r-00084.gz', 'part-r-00085.gz', 'part-r-00086.gz', 'part-r-00087.gz', 'part-r-00088.gz', 'part-r-00089.gz', 'part-r-00090.gz', 'part-r-00091.gz', 'part-r-00092.gz', 'part-r-00093.gz', 'part-r-00094.gz', 'part-r-00095.gz', 'part-r-00096.gz', 'part-r-00097.gz', 'part-r-00098.gz', 'part-r-00099.gz', 'part-r-00100.gz', 'part-r-00101.gz', 'part-r-00102.gz', 'part-r-00103.gz', 'part-r-00104.gz', 'part-r-00105.gz', 'part-r-00106.gz', 'part-r-00107.gz', 'part-r-00108.gz', 'part-r-00109.gz', 'part-r-00110.gz', 'part-r-00111.gz', 'part-r-00112.gz', 'part-r-00113.gz', 'part-r-00114.gz', 'part-r-00115.gz', 'part-r-00116.gz', 'part-r-00117.gz', 'part-r-00118.gz', 'part-r-00119.gz']\n",
      "Processing file: part-r-00000.gz\n",
      "Attempting to retrieve file: part-r-00000.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21684/2230792957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgz_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgz_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Processing file: {gz_file}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mprocess_gzip_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;31m# FTP 연결 종료\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21684/2230792957.py\u001b[0m in \u001b[0;36mprocess_gzip_file\u001b[1;34m(ftp, file_name)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;31m# 청크 단위로 데이터 읽기 (메모리 절약)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                     \u001b[1;31m# 컬럼명 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     columns = [\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# FTP 연결 설정\n",
    "ftp_host = 'varodrt.synology.me'\n",
    "ftp_port = 21\n",
    "ftp_user = '비밀'\n",
    "ftp_password = '비밀' #암호화\n",
    "\n",
    "# 서버 접속\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ftp_host, ftp_port)\n",
    "ftp.encoding = 'utf-8'\n",
    "ftp.sendcmd('OPTS UTF8 ON')  # 한글 사용을 위해 필수로 입력할 것\n",
    "ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "\n",
    "# 디렉토리 설정 및 파일 목록 가져오기\n",
    "ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/압축파일')\n",
    "file_list = ftp.nlst()  # 모든 파일 목록을 가져옴\n",
    "gz_files = [file.strip() for file in file_list if file.endswith('.gz')]  # .gz 파일만 필터링\n",
    "\n",
    "print(f\"Files found: {gz_files}\")  # 파일 목록 출력\n",
    "\n",
    "# 위도와 경도의 이상치를 제거하는 함수\n",
    "def remove_lat_lon_outliers(df):\n",
    "    df_cleaned = df[(df['GPS Y'] >= -90) & (df['GPS Y'] <= 90) &  # 위도 범위\n",
    "                    (df['GPS X'] >= -180) & (df['GPS X'] <= 180)]  # 경도 범위\n",
    "    return df_cleaned\n",
    "\n",
    "# 데이터 필터링 및 처리 함수\n",
    "def process_gzip_file(ftp, file_name):\n",
    "    all_chunks = []  # 각 청크를 담을 리스트\n",
    "\n",
    "    # FTP 작업 시 현재 디렉토리 재설정\n",
    "    try:\n",
    "        ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/압축파일')\n",
    "        with BytesIO() as bio:\n",
    "            # 파일 명을 출력해서 확인\n",
    "            print(f\"Attempting to retrieve file: {file_name}\")\n",
    "\n",
    "            try:\n",
    "                ftp.retrbinary(f\"RETR {file_name}\", bio.write)  # 파일을 메모리로 읽음\n",
    "            except ftplib.error_perm as e:\n",
    "                print(f\"FTP error while retrieving {file_name}: {e}. Skipping file.\")\n",
    "                return\n",
    "\n",
    "            bio.seek(0)\n",
    "            with gzip.open(bio, 'rt', encoding='utf-8') as f:\n",
    "                # 청크 단위로 데이터 읽기 (메모리 절약)\n",
    "                for chunk in pd.read_csv(f, delimiter='|', header=None, chunksize=1000000):\n",
    "                    # 컬럼명 설정\n",
    "                    columns = [\n",
    "                        'DTG 모델명', '차대번호', '업종', '차량번호', '사업자등록번호', '운수회사소재지코드',\n",
    "                        '운수회사코드', '일일주행거리', '누적주행거리', '운행일시', '속도', 'RPM',\n",
    "                        '브레이크신호', 'GPS X', 'GPS Y', '방위각', '가속도 X', '가속도 Y', '통신상태코드', '운행지역코드'\n",
    "                    ]\n",
    "                    chunk.columns = columns\n",
    "\n",
    "                    # 1. 차량번호가 '전북'으로 시작하는 데이터만 필터링\n",
    "                    chunk = chunk[chunk['차량번호'].str.startswith('전북')]\n",
    "\n",
    "                    # 2. 위도와 경도의 이상치 제거\n",
    "                    chunk = remove_lat_lon_outliers(chunk)\n",
    "\n",
    "                    # 3. 빈 데이터 프레임은 처리하지 않음\n",
    "                    if chunk.empty:\n",
    "                        continue\n",
    "\n",
    "                    # 4. 안전한 날짜 변환 처리\n",
    "                    chunk['운행일시'] = chunk['운행일시'].astype(str).str.slice(stop=-2)\n",
    "                    chunk.loc[:, '운행일시'] = pd.to_datetime(chunk['운행일시'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "                    chunk.loc[:, '날짜'] = chunk['운행일시'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                    # 처리된 청크를 리스트에 추가\n",
    "                    all_chunks.append(chunk)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Error while decompressing file {file_name}: {e}\")\n",
    "        return\n",
    "    except ftplib.all_errors as e:  # General FTP error handling\n",
    "        print(f\"FTP error: {e}. Skipping file: {file_name}\")\n",
    "        return\n",
    "\n",
    "    # 하나의 데이터프레임으로 합침\n",
    "    if all_chunks:\n",
    "        final_df = pd.concat(all_chunks)\n",
    "        output_filename = f\"{file_name.replace('.gz', '')}_processed.csv\"\n",
    "        upload_to_nas(ftp, final_df, output_filename)\n",
    "\n",
    "# FTP를 통해 처리된 CSV 파일을 NAS에 업로드하는 함수\n",
    "def upload_to_nas(ftp, df, output_filename):\n",
    "    if df.empty:\n",
    "        print(f\"No data to upload for {output_filename}. Skipping upload.\")\n",
    "        return\n",
    "\n",
    "    with BytesIO() as bio:\n",
    "        df.to_csv(bio, index=False, encoding='cp949')  # DataFrame을 CSV로 변환하여 메모리에 저장, 인코딩은 cp949\n",
    "        bio.seek(0)\n",
    "        try:\n",
    "            ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/처리된파일')  # 업로드할 디렉토리 경로로 이동\n",
    "            ftp.storbinary(f\"STOR {output_filename}\", bio)\n",
    "            print(f\"Successfully uploaded {output_filename} to NAS\")\n",
    "        except ftplib.all_errors as e:  # Handle all possible FTP errors\n",
    "            print(f\"FTP error while uploading {output_filename}: {e}\")\n",
    "\n",
    "# .gz 파일 처리\n",
    "for gz_file in gz_files:\n",
    "    print(f'Processing file: {gz_file}')\n",
    "    process_gzip_file(ftp, gz_file)\n",
    "\n",
    "# FTP 연결 종료\n",
    "ftp.quit()\n",
    "print(\"FTP connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de380a00-19fb-411f-9989-48674a2c8a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dda4e03f-34ae-416f-9592-68d0610d98d4",
   "metadata": {},
   "source": [
    "## 38번째 gz파일에서 decompressing data 이슈 발생하여 아래와 같이 코드를 수정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "306f46b0-8f48-49c2-ab80-d581080513a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['part-r-00042.gz', 'part-r-00043.gz']\n",
      "Processing file: part-r-00042.gz\n",
      "Attempting to retrieve file: part-r-00042.gz\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Too many columns specified: expected 20 and found 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21684/4031574713.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgz_file\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgz_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Processing file: {gz_file}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mprocess_gzip_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;31m# FTP 연결 종료\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21684/4031574713.py\u001b[0m in \u001b[0;36mprocess_gzip_file\u001b[1;34m(ftp, file_name)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[1;31m# 청크 단위로 데이터 읽기 (메모리 절약)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                     \u001b[1;31m# 컬럼명 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                     columns = [\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1072\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Too many columns specified: expected 20 and found 16"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# FTP 연결 설정\n",
    "ftp_host = 'varodrt.synology.me'\n",
    "ftp_port = 21\n",
    "ftp_user = '비밀'\n",
    "ftp_password = '비밀' #암호화\n",
    "\n",
    "# 서버 접속\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ftp_host, ftp_port)\n",
    "ftp.encoding = 'utf-8'\n",
    "ftp.sendcmd('OPTS UTF8 ON')  # 한글 사용을 위해 필수로 입력할 것\n",
    "ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "\n",
    "# 디렉토리 설정 및 파일 목록 가져오기\n",
    "ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/압축파일')\n",
    "file_list = ftp.nlst()  # 모든 파일 목록을 가져옴\n",
    "gz_files = [file.strip() for file in file_list if file.endswith('.gz')]  # .gz 파일만 필터링\n",
    "\n",
    "# 시작점 설정: 39번째 파일부터 처리\n",
    "gz_files = gz_files[39:]  # 39번째 파일 재검토\n",
    "\n",
    "print(f\"Files found: {gz_files}\")  # 파일 목록 출력\n",
    "\n",
    "# 위도와 경도의 이상치를 제거하는 함수\n",
    "def remove_lat_lon_outliers(df):\n",
    "    df_cleaned = df[(df['GPS Y'] >= -90) & (df['GPS Y'] <= 90) &  # 위도 범위\n",
    "                    (df['GPS X'] >= -180) & (df['GPS X'] <= 180)]  # 경도 범위\n",
    "    return df_cleaned\n",
    "\n",
    "# 데이터 필터링 및 처리 함수\n",
    "def process_gzip_file(ftp, file_name):\n",
    "    all_chunks = []  # 각 청크를 담을 리스트\n",
    "\n",
    "    # FTP 작업 시 현재 디렉토리 재설정\n",
    "    try:\n",
    "        ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/압축파일')\n",
    "        with BytesIO() as bio:\n",
    "            # 파일 명을 출력해서 확인\n",
    "            print(f\"Attempting to retrieve file: {file_name}\")\n",
    "\n",
    "            try:\n",
    "                ftp.retrbinary(f\"RETR {file_name}\", bio.write)  # 파일을 메모리로 읽음\n",
    "            except ftplib.error_perm as e:\n",
    "                print(f\"FTP error while retrieving {file_name}: {e}. Skipping file.\")\n",
    "                return\n",
    "\n",
    "            bio.seek(0)\n",
    "            with gzip.open(bio, 'rt', encoding='utf-8') as f:\n",
    "                # 청크 단위로 데이터 읽기 (메모리 절약)\n",
    "                for chunk in pd.read_csv(f, delimiter='|', header=None, chunksize=1000000):\n",
    "                    # 컬럼명 설정\n",
    "                    columns = [\n",
    "                        'DTG 모델명', '차대번호', '업종', '차량번호', '사업자등록번호', '운수회사소재지코드',\n",
    "                        '운수회사코드', '일일주행거리', '누적주행거리', '운행일시', '속도', 'RPM',\n",
    "                        '브레이크신호', 'GPS X', 'GPS Y', '방위각', '가속도 X', '가속도 Y', '통신상태코드', '운행지역코드'\n",
    "                    ]\n",
    "                    chunk.columns = columns\n",
    "\n",
    "                    # 1. 차량번호가 '전북'으로 시작하는 데이터만 필터링\n",
    "                    chunk = chunk[chunk['차량번호'].str.startswith('전북')]\n",
    "\n",
    "                    # 2. 위도와 경도의 이상치 제거\n",
    "                    chunk = remove_lat_lon_outliers(chunk)\n",
    "\n",
    "                    # 3. 빈 데이터 프레임은 처리하지 않음\n",
    "                    if chunk.empty:\n",
    "                        continue\n",
    "\n",
    "                    # 4. 안전한 날짜 변환 처리\n",
    "                    chunk['운행일시'] = chunk['운행일시'].astype(str).str.slice(stop=-2)\n",
    "                    chunk.loc[:, '운행일시'] = pd.to_datetime(chunk['운행일시'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "                    chunk.loc[:, '날짜'] = chunk['운행일시'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                    # 처리된 청크를 리스트에 추가\n",
    "                    all_chunks.append(chunk)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(f\"Error while decompressing file {file_name}: {e}\")\n",
    "        return\n",
    "    except ftplib.all_errors as e:  # General FTP error handling\n",
    "        print(f\"FTP error: {e}. Skipping file: {file_name}\")\n",
    "        return\n",
    "\n",
    "    # 하나의 데이터프레임으로 합침\n",
    "    if all_chunks:\n",
    "        final_df = pd.concat(all_chunks)\n",
    "        output_filename = f\"{file_name.replace('.gz', '')}_processed.csv\"\n",
    "        upload_to_nas(ftp, final_df, output_filename)\n",
    "\n",
    "# FTP를 통해 처리된 CSV 파일을 NAS에 업로드하는 함수\n",
    "def upload_to_nas(ftp, df, output_filename):\n",
    "    if df.empty:\n",
    "        print(f\"No data to upload for {output_filename}. Skipping upload.\")\n",
    "        return\n",
    "\n",
    "    with BytesIO() as bio:\n",
    "        df.to_csv(bio, index=False, encoding='cp949')  # DataFrame을 CSV로 변환하여 메모리에 저장, 인코딩은 cp949\n",
    "        bio.seek(0)\n",
    "        try:\n",
    "            ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/처리된파일')  # 업로드할 디렉토리 경로로 이동\n",
    "            ftp.storbinary(f\"STOR {output_filename}\", bio)\n",
    "            print(f\"Successfully uploaded {output_filename} to NAS\")\n",
    "        except ftplib.all_errors as e:  # Handle all possible FTP errors\n",
    "            print(f\"FTP error while uploading {output_filename}: {e}\")\n",
    "\n",
    "# .gz 파일 처리\n",
    "for gz_file in gz_files:\n",
    "    print(f'Processing file: {gz_file}')\n",
    "    process_gzip_file(ftp, gz_file)\n",
    "\n",
    "# FTP 연결 종료\n",
    "ftp.quit()\n",
    "print(\"FTP connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f03acf-5e86-4604-bd48-d3f3fa38a259",
   "metadata": {},
   "source": [
    "## Many to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ed9d475-b23a-437a-b296-71a9d289f840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read part-r-00000_processed.csv\n",
      "Successfully read part-r-00001_processed.csv\n",
      "Successfully read part-r-00004_processed.csv\n",
      "Successfully read part-r-00007_processed.csv\n",
      "Successfully read part-r-00008_processed.csv\n",
      "Successfully read part-r-00010_processed.csv\n",
      "Successfully read part-r-00011_processed.csv\n",
      "Successfully read part-r-00012_processed.csv\n",
      "Successfully read part-r-00014_processed.csv\n",
      "Successfully read part-r-00015_processed.csv\n",
      "Successfully read part-r-00016_processed.csv\n",
      "Successfully read part-r-00021_processed.csv\n",
      "Successfully read part-r-00024_processed.csv\n",
      "Successfully read part-r-00025_processed.csv\n",
      "Successfully read part-r-00026_processed.csv\n",
      "Successfully read part-r-00028_processed.csv\n",
      "Successfully read part-r-00033_processed.csv\n",
      "Successfully read part-r-00034_processed.csv\n",
      "Successfully read part-r-00035_processed.csv\n",
      "Successfully read part-r-00036_processed.csv\n",
      "Successfully read part-r-00037_processed.csv\n",
      "Successfully read part-r-00039_processed.csv\n",
      "Successfully read part-r-00044_processed.csv\n",
      "Successfully read part-r-00045_processed.csv\n",
      "Successfully read part-r-00047_processed.csv\n",
      "Successfully read part-r-00050_processed.csv\n",
      "Successfully read part-r-00051_processed.csv\n",
      "Successfully read part-r-00054_processed.csv\n",
      "Successfully read part-r-00058_processed.csv\n",
      "Successfully read part-r-00061_processed.csv\n",
      "Successfully read part-r-00063_processed.csv\n",
      "Successfully read part-r-00064_processed.csv\n",
      "Successfully read part-r-00066_processed.csv\n",
      "Successfully read part-r-00067_processed.csv\n",
      "Successfully read part-r-00068_processed.csv\n",
      "Successfully read part-r-00069_processed.csv\n",
      "Successfully read part-r-00070_processed.csv\n",
      "Successfully read part-r-00071_processed.csv\n",
      "Successfully read part-r-00072_processed.csv\n",
      "Successfully read part-r-00074_processed.csv\n",
      "Successfully read part-r-00075_processed.csv\n",
      "Successfully read part-r-00076_processed.csv\n",
      "Successfully read part-r-00079_processed.csv\n",
      "Successfully read part-r-00088_processed.csv\n",
      "Successfully read part-r-00090_processed.csv\n",
      "Successfully read part-r-00093_processed.csv\n",
      "Successfully read part-r-00095_processed.csv\n",
      "Successfully read part-r-00099_processed.csv\n",
      "Successfully read part-r-00101_processed.csv\n",
      "Successfully read part-r-00103_processed.csv\n",
      "Successfully read part-r-00104_processed.csv\n",
      "Successfully read part-r-00105_processed.csv\n",
      "Successfully read part-r-00106_processed.csv\n",
      "Successfully read part-r-00107_processed.csv\n",
      "Successfully read part-r-00108_processed.csv\n",
      "Successfully read part-r-00110_processed.csv\n",
      "Successfully read part-r-00111_processed.csv\n",
      "Successfully read part-r-00113_processed.csv\n",
      "Successfully read part-r-00116_processed.csv\n",
      "Successfully read part-r-00118_processed.csv\n",
      "Successfully concatenated 60 files.\n",
      "Successfully uploaded combined_data.csv to FTP.\n",
      "FTP connection closed.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# FTP connection settings\n",
    "ftp_host = 'varodrt.synology.me'\n",
    "ftp_port = 21\n",
    "ftp_user = '비밀'\n",
    "ftp_password = '비밀' #암호화\n",
    "# Connect to FTP server\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ftp_host, ftp_port)\n",
    "ftp.encoding = 'utf-8'\n",
    "ftp.sendcmd('OPTS UTF8 ON')  # Allow UTF-8 encoding for handling Korean characters\n",
    "ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "\n",
    "# Navigate to the directory with processed CSV files (Korean path)\n",
    "ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/처리된파일')\n",
    "\n",
    "# Get a list of all .csv files in the directory\n",
    "csv_files = [file for file in ftp.nlst() if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Function to read a CSV file from the FTP into a DataFrame\n",
    "def read_csv_from_ftp(file_name):\n",
    "    with BytesIO() as bio:\n",
    "        ftp.retrbinary(f\"RETR {file_name}\", bio.write)  # Download file into memory\n",
    "        bio.seek(0)\n",
    "        # Read CSV file into DataFrame using the likely encoding `cp949`\n",
    "        df = pd.read_csv(bio, encoding='cp949')  # Changed from 'utf-8' to 'cp949'\n",
    "        return df\n",
    "\n",
    "# Loop through each .csv file and append its DataFrame to the list\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = read_csv_from_ftp(csv_file)\n",
    "        df_list.append(df)\n",
    "        print(f\"Successfully read {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if df_list:\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Successfully concatenated {len(df_list)} files.\")\n",
    "\n",
    "    # Save the combined DataFrame into a single CSV file\n",
    "    output_filename = 'combined_data.csv'\n",
    "    with BytesIO() as bio:\n",
    "        combined_df.to_csv(bio, index=False, encoding='cp949')  # Save DataFrame as CSV to memory with 'cp949' encoding\n",
    "        bio.seek(0)\n",
    "        # Upload the concatenated file back to FTP\n",
    "        ftp.storbinary(f\"STOR {output_filename}\", bio)\n",
    "        print(f\"Successfully uploaded {output_filename} to FTP.\")\n",
    "else:\n",
    "    print(\"No data to concatenate.\")\n",
    "\n",
    "# Close the FTP connection\n",
    "ftp.quit()\n",
    "print(\"FTP connection closed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fcbdb-399f-4278-aae9-56792e08dd9a",
   "metadata": {},
   "source": [
    "## Many to 1milion rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09048972-b18c-4668-b432-7ba4b9975820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read part-r-00000_processed.csv\n",
      "Successfully read part-r-00001_processed.csv\n",
      "Successfully read part-r-00004_processed.csv\n",
      "Successfully read part-r-00007_processed.csv\n",
      "Successfully read part-r-00008_processed.csv\n",
      "Successfully read part-r-00010_processed.csv\n",
      "Successfully read part-r-00011_processed.csv\n",
      "Successfully read part-r-00012_processed.csv\n",
      "Successfully read part-r-00014_processed.csv\n",
      "Successfully read part-r-00015_processed.csv\n",
      "Successfully read part-r-00016_processed.csv\n",
      "Successfully read part-r-00021_processed.csv\n",
      "Successfully read part-r-00024_processed.csv\n",
      "Successfully read part-r-00025_processed.csv\n",
      "Successfully read part-r-00026_processed.csv\n",
      "Successfully read part-r-00028_processed.csv\n",
      "Successfully read part-r-00033_processed.csv\n",
      "Successfully read part-r-00034_processed.csv\n",
      "Successfully read part-r-00035_processed.csv\n",
      "Successfully read part-r-00036_processed.csv\n",
      "Successfully read part-r-00037_processed.csv\n",
      "Successfully read part-r-00039_processed.csv\n",
      "Successfully read part-r-00044_processed.csv\n",
      "Successfully read part-r-00045_processed.csv\n",
      "Successfully read part-r-00047_processed.csv\n",
      "Successfully read part-r-00050_processed.csv\n",
      "Successfully read part-r-00051_processed.csv\n",
      "Successfully read part-r-00054_processed.csv\n",
      "Successfully read part-r-00058_processed.csv\n",
      "Successfully read part-r-00061_processed.csv\n",
      "Successfully read part-r-00063_processed.csv\n",
      "Successfully read part-r-00064_processed.csv\n",
      "Successfully read part-r-00066_processed.csv\n",
      "Successfully read part-r-00067_processed.csv\n",
      "Successfully read part-r-00068_processed.csv\n",
      "Successfully read part-r-00069_processed.csv\n",
      "Successfully read part-r-00070_processed.csv\n",
      "Successfully read part-r-00071_processed.csv\n",
      "Successfully read part-r-00072_processed.csv\n",
      "Successfully read part-r-00074_processed.csv\n",
      "Successfully read part-r-00075_processed.csv\n",
      "Successfully read part-r-00076_processed.csv\n",
      "Successfully read part-r-00079_processed.csv\n",
      "Successfully read part-r-00088_processed.csv\n",
      "Successfully read part-r-00090_processed.csv\n",
      "Successfully read part-r-00093_processed.csv\n",
      "Successfully read part-r-00095_processed.csv\n",
      "Successfully read part-r-00099_processed.csv\n",
      "Successfully read part-r-00101_processed.csv\n",
      "Successfully read part-r-00103_processed.csv\n",
      "Successfully read part-r-00104_processed.csv\n",
      "Successfully read part-r-00105_processed.csv\n",
      "Successfully read part-r-00106_processed.csv\n",
      "Successfully read part-r-00107_processed.csv\n",
      "Successfully read part-r-00108_processed.csv\n",
      "Successfully read part-r-00110_processed.csv\n",
      "Successfully read part-r-00111_processed.csv\n",
      "Successfully read part-r-00113_processed.csv\n",
      "Successfully read part-r-00116_processed.csv\n",
      "Successfully read part-r-00118_processed.csv\n",
      "Successfully concatenated 60 files.\n",
      "Successfully uploaded combined_data_part_1.csv to FTP.\n",
      "Successfully uploaded combined_data_part_2.csv to FTP.\n",
      "Successfully uploaded combined_data_part_3.csv to FTP.\n",
      "FTP connection closed.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# FTP connection settings\n",
    "ftp_host = 'varodrt.synology.me'\n",
    "ftp_port = 21\n",
    "ftp_user = '비밀'\n",
    "ftp_password = '비밀' #암호화\n",
    "\n",
    "# Connect to FTP server\n",
    "ftp = ftplib.FTP()\n",
    "ftp.connect(ftp_host, ftp_port)\n",
    "ftp.encoding = 'utf-8'\n",
    "ftp.sendcmd('OPTS UTF8 ON')  # Allow UTF-8 encoding for handling Korean characters\n",
    "ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "\n",
    "# Navigate to the directory with processed CSV files (Korean path)\n",
    "ftp.cwd('/03. DATA Dam/전북 고속시외버스 DTG데이터(2024.6.1~2024.6.30)/처리된파일')\n",
    "\n",
    "# Get a list of all .csv files in the directory\n",
    "csv_files = [file for file in ftp.nlst() if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Function to read a CSV file from the FTP into a DataFrame\n",
    "def read_csv_from_ftp(file_name):\n",
    "    with BytesIO() as bio:\n",
    "        ftp.retrbinary(f\"RETR {file_name}\", bio.write)  # Download file into memory\n",
    "        bio.seek(0)\n",
    "        # Read CSV file into DataFrame using the likely encoding `cp949`\n",
    "        df = pd.read_csv(bio, encoding='cp949')  # Changed from 'utf-8' to 'cp949'\n",
    "        return df\n",
    "\n",
    "# Loop through each .csv file and append its DataFrame to the list\n",
    "for csv_file in csv_files:\n",
    "    try:\n",
    "        df = read_csv_from_ftp(csv_file)\n",
    "        df_list.append(df)\n",
    "        print(f\"Successfully read {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if df_list:\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Successfully concatenated {len(df_list)} files.\")\n",
    "\n",
    "    # Split and save the DataFrame into multiple CSV files, each with a max of 1 million rows\n",
    "    row_limit = 1000000  # 1 million rows per file\n",
    "    num_chunks = (len(combined_df) // row_limit) + 1  # Calculate how many files we need to create\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_row = i * row_limit\n",
    "        end_row = min(start_row + row_limit, len(combined_df))\n",
    "        chunk_df = combined_df.iloc[start_row:end_row]\n",
    "\n",
    "        # Create filename for each chunk\n",
    "        output_filename = f'combined_data_part_{i+1}.csv'\n",
    "\n",
    "        with BytesIO() as bio:\n",
    "            chunk_df.to_csv(bio, index=False, encoding='cp949')  # Save DataFrame as CSV to memory with 'cp949' encoding\n",
    "            bio.seek(0)\n",
    "            # Upload the chunked file back to FTP\n",
    "            ftp.storbinary(f\"STOR {output_filename}\", bio)\n",
    "            print(f\"Successfully uploaded {output_filename} to FTP.\")\n",
    "else:\n",
    "    print(\"No data to concatenate.\")\n",
    "\n",
    "# Close the FTP connection\n",
    "ftp.quit()\n",
    "print(\"FTP connection closed.\")\n",
    "\n",
    "# 문제가 있었던 파일 목록 출력\n",
    "if failed_files:\n",
    "    print(\"\\nThe following files encountered errors and were skipped:\")\n",
    "    for failed_file in failed_files:\n",
    "        print(f\"- {failed_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4123bf6-a0da-4a91-9f6d-f0ddfbc951f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
